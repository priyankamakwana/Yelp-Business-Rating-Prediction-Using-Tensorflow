{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import io\n",
    "import requests\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yelp_academic_dataset_review.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f203e87756e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQUOTE_MINIMAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'business_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stars'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yelp_academic_dataset_review.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# encoding added\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yelp_academic_dataset_review.json'"
     ]
    }
   ],
   "source": [
    "outfile = open(\"review_stars.tsv\", 'w')\n",
    "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "sfile.writerow(['business_id','stars', 'text'])\n",
    "with open('yelp_academic_dataset_review.json',encoding=\"utf-8\") as f:    # encoding added\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        sfile.writerow([row['business_id'], row['stars'], (row['text']).encode('utf-8')])\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c295ada0ebf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review_stars.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "df_reviews = pd.read_csv('review_stars.tsv', delimiter =\"\\t\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json to csv of business file\n",
    "\n",
    "outfile = open(\"business.tsv\", 'w')\n",
    "sfile = csv.writer(outfile, delimiter =\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "sfile.writerow(['business_id','stars', 'categories','review_count'])\n",
    "with open('yelp_academic_dataset_business.json',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        sfile.writerow([row['business_id'], row['stars'], row['categories'],row['review_count']])\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_csv('business.tsv', delimiter =\"\\t\", encoding=\"utf-8\", nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= pd.read_csv('review_grouped.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_business=pd.merge(df_business, df4, on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_numeric_range(df_review_business,'review_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.         0.0530015  0.         ... 0.         0.01715785 0.04262998]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.01986448 0.00542956 0.00323701 ... 0.01065834 0.01757678 0.03056959]\n",
      " [0.00267133 0.03285694 0.00652959 ... 0.00238885 0.00945473 0.00293638]\n",
      " [0.         0.         0.0257859  ... 0.         0.         0.06957601]]\n",
      "['00', '10', '15', '20', '30', '50', 'able', 'absolutely', 'actually', 'add', 'ago', 'amazing', 'appointment', 'area', 'arrived', 'ask', 'asked', 'ate', 'atmosphere', 'attentive', 'authentic', 'available', 'average', 'away', 'awesome', 'bacon', 'bad', 'bar', 'bbq', 'beautiful', 'beef', 'beer', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bite', 'bought', 'bowl', 'bread', 'breakfast', 'bring', 'brought', 'brunch', 'burger', 'burgers', 'burrito', 'business', 'busy', 'buy', 'cafe', 'cake', 'called', 'came', 'car', 'card', 'care', 'change', 'charge', 'cheap', 'check', 'cheese', 'chicken', 'chinese', 'chips', 'chocolate', 'choice', 'choose', 'city', 'clean', 'close', 'coffee', 'cold', 'come', 'comes', 'comfortable', 'coming', 'company', 'completely', 'cooked', 'cool', 'cost', 'couldn', 'counter', 'couple', 'course', 'cream', 'crispy', 'customer', 'customers', 'cut', 'day', 'days', 'deal', 'decent', 'decided', 'decor', 'definitely', 'delicious', 'dessert', 'did', 'didn', 'different', 'dining', 'dinner', 'disappointed', 'dish', 'dishes', 'does', 'doesn', 'dog', 'doing', 'don', 'door', 'dr', 'drink', 'drinks', 'drive', 'dry', 'early', 'easy', 'eat', 'eating', 'egg', 'eggs', 'employees', 'end', 'ended', 'enjoy', 'enjoyed', 'entire', 'especially', 'evening', 'excellent', 'expect', 'expected', 'expensive', 'experience', 'extra', 'extremely', 'fact', 'family', 'fan', 'fantastic', 'far', 'fast', 'favorite', 'feel', 'felt', 'finally', 'fine', 'fish', 'flavor', 'flavors', 'floor', 'food', 'free', 'french', 'fresh', 'friday', 'fried', 'friend', 'friendly', 'friends', 'fries', 'fun', 'gave', 'gets', 'getting', 'girl', 'given', 'glad', 'going', 'gone', 'good', 'got', 'great', 'green', 'grilled', 'group', 'guess', 'guy', 'guys', 'hair', 'half', 'happy', 'hard', 'haven', 'having', 'heard', 'help', 'helpful', 'high', 'highly', 'home', 'horrible', 'hot', 'hotel', 'hour', 'hours', 'house', 'huge', 'husband', 'ice', 'impressed', 'ingredients', 'inside', 'instead', 'isn', 'issue', 'italian', 'items', 'job', 'just', 'kept', 'kids', 'kind', 'kitchen', 'knew', 'know', 'la', 'lady', 'large', 'las', 'late', 'later', 'leave', 'left', 'let', 'life', 'light', 'like', 'liked', 'line', 'list', 'little', 'live', 'll', 'local', 'location', 'long', 'look', 'looked', 'looking', 'looks', 'lot', 'lots', 'love', 'loved', 'lunch', 'main', 'make', 'makes', 'making', 'mall', 'man', 'manager', 'massage', 'maybe', 'meal', 'meat', 'menu', 'mexican', 'mind', 'minutes', 'money', 'month', 'months', 'morning', 'moved', 'music', 'near', 'need', 'needed', 'needs', 'new', 'ni', 'nice', 'nif', 'night', 'nit', 'nmy', 'non', 'nthe', 'nthey', 'nthis', 'number', 'nwe', 'offer', 'offered', 'office', 'oh', 'ok', 'okay', 'old', 'open', 'option', 'options', 'order', 'ordered', 'ordering', 'orders', 'outside', 'overall', 'owner', 'paid', 'parking', 'party', 'past', 'pasta', 'patio', 'pay', 'people', 'perfect', 'perfectly', 'person', 'phone', 'pick', 'pizza', 'place', 'places', 'plate', 'pleasant', 'plenty', 'plus', 'point', 'pool', 'pork', 'portion', 'portions', 'potatoes', 'pretty', 'price', 'priced', 'prices', 'probably', 'problem', 'professional', 'quality', 'quick', 'quickly', 'quite', 'ready', 'real', 'really', 'reason', 'reasonable', 'received', 'recently', 'recommend', 'recommended', 'red', 'regular', 'remember', 'restaurant', 'restaurants', 'return', 'review', 'reviews', 'rice', 'right', 'roll', 'rolls', 'room', 'rooms', 'rude', 'run', 'said', 'salad', 'salon', 'sandwich', 'sandwiches', 'sat', 'saturday', 'sauce', 'saw', 'say', 'seated', 'seating', 'second', 'seen', 'selection', 'serve', 'served', 'server', 'service', 'set', 'shop', 'short', 'shrimp', 'simple', 'sit', 'sitting', 'size', 'slow', 'small', 'soon', 'soup', 'space', 'special', 'spicy', 'spot', 'staff', 'star', 'stars', 'start', 'started', 'stay', 'steak', 'stop', 'stopped', 'store', 'street', 'strip', 'stuff', 'style', 'sunday', 'super', 'sure', 'sushi', 'sweet', 'table', 'tables', 'taco', 'tacos', 'taken', 'taking', 'taste', 'tasted', 'tasty', 'tea', 'tell', 'terrible', 'thai', 'thank', 'thanks', 'thing', 'things', 'think', 'thought', 'time', 'times', 'tip', 'today', 'told', 'took', 'totally', 'town', 'tried', 'trip', 'try', 'trying', 'twice', 'type', 'understand', 'use', 'used', 'usually', 'variety', 've', 'vegas', 'visit', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'walk', 'walked', 'want', 'wanted', 'warm', 'wasn', 'water', 'way', 'week', 'weekend', 'weeks', 'went', 'white', 'wife', 'wine', 'wings', 'wish', 'won', 'wonderful', 'work', 'working', 'worst', 'worth', 'wouldn', 'wrong', 'xc3', 'year', 'years', 'yelp', 'yes', 'yummy']\n"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text as sk_text\n",
    "vectorizer = sk_text.TfidfVectorizer(stop_words='english',\n",
    "                            max_features = 500,\n",
    "                            min_df=1\n",
    "                            #max_df=5\n",
    "                            )\n",
    "\n",
    "#min_df: ignore terms that have a document frequency < min_df.\n",
    "#max_df: ignore terms that have a document frequency > max_df\n",
    "\n",
    "\n",
    "matrix = vectorizer.fit_transform(df_review_business['all_reviews'])\n",
    "print(type(matrix))               # Compressed Sparse Row matrix\n",
    "\n",
    "tfidf_data = matrix.toarray()     #  convert it to numpy array\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "print(tfidf_data)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 500)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting review_count in Numpy Array\n",
    "review_array=df_review_business['review_count'].values\n",
    "review_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 501)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting review_count array into vertical stack\n",
    "vertical_review=np.vstack(review_array)\n",
    "#Concatenating review_text tfidf array and review_count array\n",
    "x = np.concatenate((tfidf_data, vertical_review), axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df_review_business['stars']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_id = df_business['business_id'][0:5]\n",
    "b_id\n",
    "testx = x[0:5]\n",
    "x=x[5:10000]\n",
    "testy = y[0:5]\n",
    "y=y[5:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Plot a confusion matrix.\n",
    "# cm is the confusion matrix, names are the names of the classes.\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7496 samples, validate on 2499 samples\n",
      "Epoch 1/1000\n",
      " - 2s - loss: 4.1435 - val_loss: 1.1681\n",
      "Epoch 2/1000\n",
      " - 1s - loss: 0.8002 - val_loss: 0.4956\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 0.4186 - val_loss: 0.3672\n",
      "Epoch 4/1000\n",
      " - 1s - loss: 0.3502 - val_loss: 0.3397\n",
      "Epoch 5/1000\n",
      " - 1s - loss: 0.3230 - val_loss: 0.3256\n",
      "Epoch 6/1000\n",
      " - 1s - loss: 0.3075 - val_loss: 0.3192\n",
      "Epoch 7/1000\n",
      " - 1s - loss: 0.2968 - val_loss: 0.3185\n",
      "Epoch 8/1000\n",
      " - 1s - loss: 0.2910 - val_loss: 0.3151\n",
      "Epoch 9/1000\n",
      " - 1s - loss: 0.2859 - val_loss: 0.3184\n",
      "Epoch 10/1000\n",
      " - 1s - loss: 0.2821 - val_loss: 0.3154\n",
      "Epoch 11/1000\n",
      " - 1s - loss: 0.2801 - val_loss: 0.3167\n",
      "Epoch 12/1000\n",
      " - 1s - loss: 0.2779 - val_loss: 0.3189\n",
      "Epoch 13/1000\n",
      " - 1s - loss: 0.2745 - val_loss: 0.3169\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# 25 neurons in 1st hidden layer\n",
    "model.add(Dense(25, input_dim=x_train.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
    "# 10 neurons in 2nd hidden layer\n",
    "model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "#optimizer - Back Prop algo\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "model.fit(x_train,y_train, validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)\n",
    "\n",
    "model.load_weights('best_weights.hdf5') # load weights from best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 7496 samples, validate on 2499 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 2.5919 - val_loss: 0.4617\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.3799 - val_loss: 0.3338\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.3176 - val_loss: 0.3163\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.2981 - val_loss: 0.3155\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.2907 - val_loss: 0.3152\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.2857 - val_loss: 0.3207\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.2697 - val_loss: 0.3323\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.2480 - val_loss: 0.2979\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.2227 - val_loss: 0.2925\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.2023 - val_loss: 0.2919\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.1832 - val_loss: 0.2974\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.1701 - val_loss: 0.2930\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.1567 - val_loss: 0.2987\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.1452 - val_loss: 0.3015\n",
      "Epoch 00014: early stopping\n",
      "1\n",
      "Train on 7496 samples, validate on 2499 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.6636 - val_loss: 0.5960\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.4110 - val_loss: 0.3404\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.3242 - val_loss: 0.3209\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.3012 - val_loss: 0.3141\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.2890 - val_loss: 0.3129\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.2807 - val_loss: 0.3080\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.2679 - val_loss: 0.2992\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.2518 - val_loss: 0.2992\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.2383 - val_loss: 0.2820\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.2243 - val_loss: 0.2810\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.2125 - val_loss: 0.2793\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.2031 - val_loss: 0.2874\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.1952 - val_loss: 0.2836\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.1879 - val_loss: 0.2885\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.1776 - val_loss: 0.2872\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.1714 - val_loss: 0.2935\n",
      "Epoch 00016: early stopping\n",
      "2\n",
      "Train on 7496 samples, validate on 2499 samples\n",
      "Epoch 1/100\n",
      " - 3s - loss: 3.0625 - val_loss: 0.7540\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.4980 - val_loss: 0.3689\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.3456 - val_loss: 0.3326\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.3132 - val_loss: 0.3216\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.2950 - val_loss: 0.3181\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.2857 - val_loss: 0.3121\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.2723 - val_loss: 0.3115\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.2557 - val_loss: 0.2988\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.2402 - val_loss: 0.2960\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.2249 - val_loss: 0.2904\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.2120 - val_loss: 0.2924\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.2006 - val_loss: 0.2869\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.1897 - val_loss: 0.3088\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.1771 - val_loss: 0.2900\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.1655 - val_loss: 0.2954\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.1533 - val_loss: 0.3018\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.1433 - val_loss: 0.3032\n",
      "Epoch 00017: early stopping\n",
      "3\n",
      "Train on 7496 samples, validate on 2499 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 2.8319 - val_loss: 0.6912\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.4553 - val_loss: 0.3575\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.3339 - val_loss: 0.3232\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.3067 - val_loss: 0.3154\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.2936 - val_loss: 0.3195\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.2854 - val_loss: 0.3144\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.2810 - val_loss: 0.3142\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.2736 - val_loss: 0.3094\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.2653 - val_loss: 0.3115\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.2462 - val_loss: 0.2972\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.2287 - val_loss: 0.2933\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.2147 - val_loss: 0.2893\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.2008 - val_loss: 0.2883\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.1911 - val_loss: 0.2871\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.1827 - val_loss: 0.2882\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.1746 - val_loss: 0.2923\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.1648 - val_loss: 0.2991\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.1560 - val_loss: 0.2981\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.1485 - val_loss: 0.3014\n",
      "Epoch 00019: early stopping\n",
      "4\n",
      "Train on 7496 samples, validate on 2499 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 2.3581 - val_loss: 0.6076\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.4318 - val_loss: 0.3546\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.3284 - val_loss: 0.3287\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.3029 - val_loss: 0.3199\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.2876 - val_loss: 0.3111\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.2703 - val_loss: 0.3039\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.2557 - val_loss: 0.2990\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.2422 - val_loss: 0.2926\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.2294 - val_loss: 0.2925\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.2165 - val_loss: 0.2952\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.2037 - val_loss: 0.3027\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.1893 - val_loss: 0.3000\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.1795 - val_loss: 0.3056\n",
      "Epoch 00013: early stopping\n",
      "Training finished...Loading the best model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define ModelCheckpoint outside the loop\n",
    "\n",
    "# Remove checkpointer and loop to see change in result\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "\n",
    "    # Build network\n",
    "    model = Sequential()\n",
    "    # 25 neurons in 1st hidden layer\n",
    "    model.add(Dense(25, input_dim=x_train.shape[1], activation='relu')) # Hidden 1     #  why input_dim=x.shape[1]?  \n",
    "    # 10 neurons in 2nd hidden layer\n",
    "    model.add(Dense(10, activation='relu')) # Hidden 2\n",
    "    model.add(Dense(5, activation='relu')) # Hidden 3\n",
    "    model.add(Dense(1)) # Output\n",
    "    #optimizer - Back Prop algo\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=100)\n",
    "\n",
    "\n",
    "print('Training finished...Loading the best model')  \n",
    "print()\n",
    "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
